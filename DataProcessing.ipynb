{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fc510ae",
   "metadata": {},
   "source": [
    "tldr - This data has been downloaded from my \"saved jobs\" on LinkedIn. This notebook helps process and sort the data into our groups for the experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64790c1",
   "metadata": {},
   "source": [
    "# Summary\n",
    "I'm running an experiment on how well two different resume's perform. Does a professionally written resume perform better than one I've written myself?\t\t\t\n",
    "\n",
    "# Hypothesis\n",
    "$H_{0} = \\text{The resume and cover letter provided by Haley Stock perform the same as my own self-written resume and cover letter.}$\n",
    "\n",
    "$H_{a} = \\text{The professionally-written resume and cover letter have a different interview invitation rate than self-written resume and cover letter.}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb8b8a2",
   "metadata": {},
   "source": [
    "| Name                       | Definition                                                                                                   | Example Format | P1   | P2 (from detectable difference below) |\n",
    "|----------------------------|-------------------------------------------------------------------------------------------------------------|----------------|------|----------------------------------------|\n",
    "| Interview Invitation Rate  | This is the percent of applications that receive an interview invite. It shows how interested companies are in my resume. | 0.50%         | 8%   | 13%                                    |\n",
    "| Application Response Rate  | This is the percent of applications that receive a rejection/interview invite within 48 hrs. It shows how confident companies are in my qualifications. | 1%             |      |                                        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5fda4e",
   "metadata": {},
   "source": [
    "# Data Collection Process\n",
    "**LinkedIn:** I searched for \"data\" and remote in US roles. I then saved the roles that met the criteria below to a csv (via LinkedIn's data request form)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5e6da3",
   "metadata": {},
   "source": [
    "# Inclusion Criteria\n",
    "Here are the kinds of jobs I'm interested in. I'm going to include these roles in the experiment based on the logic listed below.\n",
    "\n",
    "| **Category**            | **Details**                                                                                                                                                          |\n",
    "|:------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **Location**            | Remote work only                                                                                                                                                     |\n",
    "| **Minimum Job Roles**   | Data Scientist, Senior Data/Product Analyst, or Manager/Lead<br>No Applied Scientist, Decision Science, Analytics Engineer, or Researcher roles (significantly different than my experience would allow)<br>No Product Manager roles (not interested in product management)<br>No Director level roles (largely above my level) |\n",
    "| **Type**                | Full time and contracted                                                                                                                                             |\n",
    "| **Duplicate Job Roles** | Sometimes companies post multiple versions of a job (by location or reposts after a few weeks have gone by). Will do my best to dedupe these in the final \"saved jobs\" list. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d63981e",
   "metadata": {},
   "source": [
    "# Potential Pitfalls and Adjustments\n",
    "1) *Cutting sample collection short.* **Discussion:** See power analysis below. Peer review pending.\n",
    "2) *Confirming random assignment.* **Discussion:** Randomizing at cluster level (company + job title).\n",
    "3) *Random assignment fails to equally distribute \"heavy\" segments across A/B.* **Discussion:** Equally distribute the following segments and assess post-assignment:\n",
    "   - Recency of job posting\n",
    "   - \"Repeat\" companies (see \"8)\" below)\n",
    "4) *Ensure the Ceteris Parabus assumption* (that is, ensure all else is same between treatment/control besides the treatment). **Discussion:** Use the exact resume and cover letter provided for both.\n",
    "5) *Cross-contamination for treatment and control groups.* **Discussion:** I don't think we'll have to worry about this, unless a recruiter leaves one company and goes to another.\n",
    "6) *Multiple comparisons can lead to higher false positive rate.* **Discussion:** Only making one comparison -- professionally written resume vs. self-written resume.\n",
    "7) *Simpson's paradox due to graduated roll-outs.* **Discussion:** I'm only performing one testing period, so this point is moot. Could simply look at final period if so.\n",
    "8) *Primacy or novelty effect.* **Discussion:** This could be a problem, if I previously applied for a role with a company before conducting the experiment. I can adjust for this by running the experiment longer, but that's not really helpful given my current situation. Instead, I will include a category segment across treatment/control for repeat and new companies.\n",
    "    - \"Repeat\" = companies that I've applied to before. I'll attempt to equally distribute these during assignment.\n",
    "    - \"New\" = companies that I have not applied to before. I'll attempt to equally distribute these during assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80df2ab6",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d38d3e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3626.59s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "/notebooks/resume_experiment\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bbaeabdc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/resume_experiment/data/modified_data.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "#import the job sampling data\n",
    "data_folder = \"/notebooks/resume_experiment/data\"\n",
    "dataf = \"modified_data.csv\"\n",
    "data_file = os.path.join(data_folder, dataf)\n",
    "\n",
    "print(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "37bfa8ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>role_title</th>\n",
       "      <th>company</th>\n",
       "      <th>date_saved</th>\n",
       "      <th>posting_url</th>\n",
       "      <th>days_since_post</th>\n",
       "      <th>is_repeat_company</th>\n",
       "      <th>date_posted</th>\n",
       "      <th>Applied Jobs Check</th>\n",
       "      <th>DELETE Row</th>\n",
       "      <th>Check Unique Job/Co</th>\n",
       "      <th>job_posted_pin</th>\n",
       "      <th>role_cat</th>\n",
       "      <th>sorting_hat_col</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Advisor, Data Science and Advance Analytics</td>\n",
       "      <td>Ciena</td>\n",
       "      <td>11/9/24, 10:51 AM</td>\n",
       "      <td>http://www.linkedin.com/jobs/view/4059612928</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11/9/2024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>7days</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>0.0_7days_Data Scientist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Sr. Manager, Insights &amp; Analytics - Customer S...</td>\n",
       "      <td>Dyson</td>\n",
       "      <td>11/9/24, 10:39 AM</td>\n",
       "      <td>http://www.linkedin.com/jobs/view/4040507479</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11/13/2024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>7days</td>\n",
       "      <td>Manager</td>\n",
       "      <td>1.0_7days_Manager</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Provation</td>\n",
       "      <td>11/10/24, 11:25 AM</td>\n",
       "      <td>http://www.linkedin.com/jobs/view/4048274523</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11/8/2024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>7days</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>0.0_7days_Data Scientist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>Senior Manager, Data and Analytics - Product</td>\n",
       "      <td>TriNet</td>\n",
       "      <td>11/5/24, 9:06 AM</td>\n",
       "      <td>http://www.linkedin.com/jobs/view/4049635016</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11/8/2024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>7days</td>\n",
       "      <td>Manager</td>\n",
       "      <td>0.0_7days_Manager</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>Senior Product Analyst</td>\n",
       "      <td>Greenlight</td>\n",
       "      <td>11/8/24, 8:39 AM</td>\n",
       "      <td>http://www.linkedin.com/jobs/view/4055136369</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10/25/2024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>30days</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>0.0_30days_Data Analyst</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            role_title     company  \\\n",
       "100        Advisor, Data Science and Advance Analytics       Ciena   \n",
       "16   Sr. Manager, Insights & Analytics - Customer S...       Dyson   \n",
       "176                                     Data Scientist   Provation   \n",
       "246       Senior Manager, Data and Analytics - Product      TriNet   \n",
       "263                             Senior Product Analyst  Greenlight   \n",
       "\n",
       "             date_saved                                   posting_url  \\\n",
       "100   11/9/24, 10:51 AM  http://www.linkedin.com/jobs/view/4059612928   \n",
       "16    11/9/24, 10:39 AM  http://www.linkedin.com/jobs/view/4040507479   \n",
       "176  11/10/24, 11:25 AM  http://www.linkedin.com/jobs/view/4048274523   \n",
       "246    11/5/24, 9:06 AM  http://www.linkedin.com/jobs/view/4049635016   \n",
       "263    11/8/24, 8:39 AM  http://www.linkedin.com/jobs/view/4055136369   \n",
       "\n",
       "     days_since_post  is_repeat_company date_posted Applied Jobs Check  \\\n",
       "100              6.0                0.0   11/9/2024                NaN   \n",
       "16               2.0                1.0  11/13/2024                NaN   \n",
       "176              7.0                0.0   11/8/2024                NaN   \n",
       "246              7.0                0.0   11/8/2024                NaN   \n",
       "263             21.0                0.0  10/25/2024                NaN   \n",
       "\n",
       "    DELETE Row  Check Unique Job/Co job_posted_pin        role_cat  \\\n",
       "100        NaN                    1          7days  Data Scientist   \n",
       "16         NaN                    1          7days         Manager   \n",
       "176        NaN                    2          7days  Data Scientist   \n",
       "246        NaN                    1          7days         Manager   \n",
       "263        NaN                    1         30days    Data Analyst   \n",
       "\n",
       "              sorting_hat_col  \n",
       "100  0.0_7days_Data Scientist  \n",
       "16          1.0_7days_Manager  \n",
       "176  0.0_7days_Data Scientist  \n",
       "246         0.0_7days_Manager  \n",
       "263   0.0_30days_Data Analyst  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>role_title</th>\n",
       "      <th>company</th>\n",
       "      <th>date_saved</th>\n",
       "      <th>posting_url</th>\n",
       "      <th>days_since_post</th>\n",
       "      <th>is_repeat_company</th>\n",
       "      <th>date_posted</th>\n",
       "      <th>Applied Jobs Check</th>\n",
       "      <th>DELETE Row</th>\n",
       "      <th>Check Unique Job/Co</th>\n",
       "      <th>job_posted_pin</th>\n",
       "      <th>role_cat</th>\n",
       "      <th>sorting_hat_col</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [role_title, company, date_saved, posting_url, days_since_post, is_repeat_company, date_posted, Applied Jobs Check, DELETE Row, Check Unique Job/Co, job_posted_pin, role_cat, sorting_hat_col]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_df = pd.read_csv(data_file, low_memory=False)\n",
    "raw_df = raw_df.dropna(axis=0,subset=\"role_title\")\n",
    "raw_df = raw_df.loc[raw_df.loc[:,'DELETE Row'] != True]\n",
    "raw_df['sorting_hat_col'] = raw_df['is_repeat_company'].astype('str')+\"_\"+raw_df['job_posted_pin']+\"_\"+raw_df['role_cat']\n",
    "\n",
    "display(raw_df.sample(5))\n",
    "display(raw_df.loc[raw_df.loc[:,\"sorting_hat_col\"].isnull()].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bd9f5359",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>is_repeat_company</th>\n",
       "      <th>counts</th>\n",
       "      <th>days_since_post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6sense</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8x8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abercrombie &amp; Fitch Co.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abnormal Security</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Age of Learning</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   company  is_repeat_company  counts  days_since_post\n",
       "0                   6sense                0.0       1              7.0\n",
       "1                      8x8                0.0       1             14.0\n",
       "2  Abercrombie & Fitch Co.                0.0       1             14.0\n",
       "3        Abnormal Security                0.0       1             14.0\n",
       "4          Age of Learning                0.0       1              6.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#build a list of the companies that are in our dataset. include new vs. repeat as a dimension\n",
    "companies = raw_df.copy().filter(['company','is_repeat_company','days_since_post'])\n",
    "companies['counts'] = 1\n",
    "agg_mets = {'counts':\"count\", 'days_since_post':\"mean\"}\n",
    "companies = companies.groupby(['company','is_repeat_company']).agg(agg_mets).reset_index()\n",
    "display(companies.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a6163b44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ensure company lists are mutually exclusive\n",
    "new_companies = set(new_cluster['company'])\n",
    "repeat_companies = set(repeat_cluster['company'])\n",
    "\n",
    "is_mutually_exclusive = new_companies.isdisjoint(repeat_cluster)\n",
    "assert is_mutually_exclusive, \"Companies overlapp between new and repeat.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e73449ad-34e5-429b-b0c7-fbf64e302edd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Randomly assign clusters to groups\u001b[39;00m\n\u001b[1;32m      2\u001b[0m sorting_hat \u001b[38;5;241m=\u001b[39m StratifiedShuffleSplit(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ctrl, trt \u001b[38;5;129;01min\u001b[39;00m \u001b[43msorting_hat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msorting_hat_col\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m      5\u001b[0m     ctrl_grp \u001b[38;5;241m=\u001b[39m raw_df\u001b[38;5;241m.\u001b[39miloc[ctrl]\n\u001b[1;32m      6\u001b[0m     trt_grp \u001b[38;5;241m=\u001b[39m raw_df\u001b[38;5;241m.\u001b[39miloc[trt]\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:2197\u001b[0m, in \u001b[0;36mStratifiedShuffleSplit.split\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   2163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, groups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   2164\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate indices to split data into training and test set.\u001b[39;00m\n\u001b[1;32m   2165\u001b[0m \n\u001b[1;32m   2166\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2195\u001b[0m \u001b[38;5;124;03m    to an integer.\u001b[39;00m\n\u001b[1;32m   2196\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2197\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msplit(X, y, groups)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:959\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    953\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    954\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    955\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    956\u001b[0m         )\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 959\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    967\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:109\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nan:\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _object_dtype_isnan(X)\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m--> 109\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput contains NaN\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# We need only consider float arrays, hence can early return for all else.\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m xp\u001b[38;5;241m.\u001b[39misdtype(X\u001b[38;5;241m.\u001b[39mdtype, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreal floating\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomplex floating\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN"
     ]
    }
   ],
   "source": [
    "#Randomly assign clusters to groups\n",
    "sorting_hat = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "\n",
    "for ctrl, trt in sorting_hat.split(raw_df, raw_df['sorting_hat_col']):\n",
    "    ctrl_grp = raw_df.iloc[ctrl]\n",
    "    trt_grp = raw_df.iloc[trt]\n",
    "    \n",
    "# Ensure balanced distribution\n",
    "print(\"Treatment Group Distribution:\")\n",
    "print(trt_grp['stratify_col'].value_counts())\n",
    "\n",
    "print(\"\\nControl Group Distribution:\")\n",
    "print(ctrl_grp['stratify_col'].value_counts())\n",
    "\n",
    "# Treatment and control groups\n",
    "print(\"\\nTreatment Group Sample:\")\n",
    "print(trt_grp.head())\n",
    "\n",
    "print(\"\\nControl Group Sample:\")\n",
    "print(ctrl_grp.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2ddcb5cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2746964248.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[40], line 6\u001b[0;36m\u001b[0m\n\u001b[0;31m    generate random number between 1,2\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for c in clusters:\n",
    "    generate random number between 1,2\n",
    "    if random number == 1:\n",
    "        assign c to control\n",
    "    elif random number == 2:\n",
    "        assign c to treatment\n",
    "\n",
    "save the assignment to a file for control\n",
    "save the assignment to a file for treatment\n",
    "tell me how many jobs are in each treatment and control cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98dd3a5-0d2c-4b67-a102-463d5ba56bfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
