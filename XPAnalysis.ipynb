{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fc510ae",
   "metadata": {},
   "source": [
    "tldr - This data has been downloaded from my \"saved jobs\" on LinkedIn. This notebook helps process and sort the data into our groups for the experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64790c1",
   "metadata": {},
   "source": [
    "# Summary\n",
    "I'm running an experiment on how well two different resume's perform. Does a professionally written resume perform better than one I've written myself?\t\t\t\n",
    "\n",
    "# Hypothesis\n",
    "$H_{0} = \\text{The resume and cover letter provided by Haley Stock perform the same as my own self-written resume and cover letter.}$\n",
    "\n",
    "$H_{a} = \\text{The professionally-written resume and cover letter have a different interview invitation rate than self-written resume and cover letter.}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb8b8a2",
   "metadata": {},
   "source": [
    "| Name                       | Definition                                                                                                   | Example Format | P1   | P2 (from detectable difference below) |\n",
    "|----------------------------|-------------------------------------------------------------------------------------------------------------|----------------|------|----------------------------------------|\n",
    "| Interview Invitation Rate  | This is the percent of applications that receive an interview invite. It shows how interested companies are in my resume. | 0.50%         | 8%   | 13%                                    |\n",
    "| Application Response Rate  | This is the percent of applications that receive a rejection/interview invite within 48 hrs. It shows how confident companies are in my qualifications. | 1%             |      |                                        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5fda4e",
   "metadata": {},
   "source": [
    "# Data Collection Process\n",
    "**LinkedIn:** I searched for \"data\" and remote in US roles. I then saved the roles that met the criteria below to a csv (via LinkedIn's data request form)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5e6da3",
   "metadata": {},
   "source": [
    "# Inclusion Criteria\n",
    "Here are the kinds of jobs I'm interested in. I'm going to include these roles in the experiment based on the logic listed below.\n",
    "\n",
    "| **Category**            | **Details**                                                                                                                                                          |\n",
    "|:------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **Location**            | Remote work only                                                                                                                                                     |\n",
    "| **Minimum Job Roles**   | Data Scientist, Senior Data/Product Analyst, or Manager/Lead<br>No Applied Scientist, Decision Science, Analytics Engineer, or Researcher roles (significantly different than my experience would allow)<br>No Product Manager roles (not interested in product management)<br>No Director level roles (largely above my level) <br>Minimum salary of $100k/yr|\n",
    "| **Type**                | Full time and contracted                                                                                                                                             |\n",
    "| **Duplicate Job Roles** | Sometimes companies post multiple versions of a job (by location or reposts after a few weeks have gone by). Will do my best to dedupe these in the final \"saved jobs\" list. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d63981e",
   "metadata": {},
   "source": [
    "# Potential Pitfalls and Adjustments\n",
    "1) *Cutting sample collection short.* **Discussion:** See power analysis below. Peer review pending.\n",
    "2) *Confirming random assignment.* **Discussion:** We will randomizing at cluster level (repeat/not yet applied company + Analyst/Scientist/or Manager role).\n",
    "3) *Random assignment fails to equally distribute \"heavy\" segments across A/B.* **Discussion:** Equally distribute the following segments and assess post-assignment:\n",
    "   - Recency of job posting\n",
    "   - \"Repeat\" companies (see \"8)\" below)\n",
    "   - Role type: Analyst, Scientist, or Manager\n",
    "4) *Ensure the Ceteris Parabus assumption* (that is, ensure all else is same between treatment/control besides the treatment). **Discussion:** Use the exact resume and cover letter provided for both.\n",
    "5) *Cross-contamination for treatment and control groups.* **Discussion:** This could be an issue if different role types have the same recruiter.\n",
    "6) *Multiple comparisons can lead to higher false positive rate.* **Discussion:** Only making one comparison -- professionally written resume vs. self-written resume.\n",
    "7) *Simpson's paradox due to graduated roll-outs.* **Discussion:** I'm only performing one testing period, so this point is moot. Could simply look at final period if so.\n",
    "8) *Primacy or novelty effect.* **Discussion:** This could be a problem, if I previously applied for a role with a company before conducting the experiment. I can adjust for this by running the experiment longer, but that's not really helpful given my current situation. Instead, I will include a category segment across treatment/control for repeat and new companies.\n",
    "    - \"Repeat\" = companies that I've applied to before. I'll attempt to equally distribute these during assignment.\n",
    "    - \"New\" = companies that I have not applied to before. I'll attempt to equally distribute these during assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80df2ab6",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "\n",
    "I have compiled the data with responses into a file `'data/total_split.csv'`. It contains every applied job posting along with a date that I received an interview. \n",
    "\n",
    "We've reached the original sample size of 350 for control and treatment. However, the response rate is far lower than expected. I will still perform the power analysis on this collection of data. However, it's very likely the difference won't be stat sig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac76a199-8c3a-46ba-9905-cc1815161604",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>role_title</th>\n",
       "      <th>company</th>\n",
       "      <th>date_saved</th>\n",
       "      <th>posting_url</th>\n",
       "      <th>days_since_post</th>\n",
       "      <th>is_repeat_company</th>\n",
       "      <th>date_posted</th>\n",
       "      <th>job_posted_pin</th>\n",
       "      <th>role_cat</th>\n",
       "      <th>sorting_hat_col</th>\n",
       "      <th>ab_split</th>\n",
       "      <th>Link</th>\n",
       "      <th>Applied Date \\n(Blank if not applied)</th>\n",
       "      <th>Deletion details</th>\n",
       "      <th>Interview Invite</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>51</td>\n",
       "      <td>Lead Data Scientist - Predictive Analytics - 2...</td>\n",
       "      <td>Medix Technology</td>\n",
       "      <td>12/4/2024</td>\n",
       "      <td>http://www.linkedin.com/jobs/view/4088269688</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Manager</td>\n",
       "      <td>0.0_Manager</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4088269688/...</td>\n",
       "      <td>12/5/2024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>86</td>\n",
       "      <td>Principle Data Scientist - Experimentation</td>\n",
       "      <td>Atlassian</td>\n",
       "      <td>11/9/24, 10:40 AM</td>\n",
       "      <td>http://www.linkedin.com/jobs/view/4066488214</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>11/8/2024</td>\n",
       "      <td>7days</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>1.0_Data Scientist</td>\n",
       "      <td>0</td>\n",
       "      <td>https://careers-americas.icims.com/jobs/15903/...</td>\n",
       "      <td>12/02/2024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                         role_title  \\\n",
       "547          51  Lead Data Scientist - Predictive Analytics - 2...   \n",
       "253          86         Principle Data Scientist - Experimentation   \n",
       "\n",
       "              company         date_saved  \\\n",
       "547  Medix Technology          12/4/2024   \n",
       "253         Atlassian  11/9/24, 10:40 AM   \n",
       "\n",
       "                                      posting_url  days_since_post  \\\n",
       "547  http://www.linkedin.com/jobs/view/4088269688              NaN   \n",
       "253  http://www.linkedin.com/jobs/view/4066488214              7.0   \n",
       "\n",
       "     is_repeat_company date_posted job_posted_pin        role_cat  \\\n",
       "547                  0         NaN            NaN         Manager   \n",
       "253                  1   11/8/2024          7days  Data Scientist   \n",
       "\n",
       "        sorting_hat_col  ab_split  \\\n",
       "547         0.0_Manager         0   \n",
       "253  1.0_Data Scientist         0   \n",
       "\n",
       "                                                  Link  \\\n",
       "547  https://www.linkedin.com/jobs/view/4088269688/...   \n",
       "253  https://careers-americas.icims.com/jobs/15903/...   \n",
       "\n",
       "    Applied Date \\n(Blank if not applied) Deletion details Interview Invite  \n",
       "547                             12/5/2024              NaN              NaN  \n",
       "253                            12/02/2024              NaN              NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm, t\n",
    "\n",
    "#Loading and renaming cols\n",
    "raw_samples = pd.read_csv('data/total_split.csv')\n",
    "display(raw_samples.sample(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98ec9979-f29f-4399-845c-bff395534418",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calcRR(df):\n",
    "    df = df.copy()[['ab_split','response_yn']].groupby('ab_split').agg(['sum','count']).reset_index()\n",
    "    df.columns = ['ab_split','sum','count']\n",
    "    df['response_rate'] = df['sum'].div(df['count'])\n",
    "    return(df)\n",
    "\n",
    "class PowerAnalysis:\n",
    "    \"\"\"A class to represent a power analysis.\"\"\"\n",
    "    \n",
    "    def __init__(self,raw_data):\n",
    "        \"\"\"Initialize the power analysis object.\"\"\"\n",
    "        self.raw_data = raw_data\n",
    "        \n",
    "    def proc_rawdata(self, rm_null_rows=\"Applied Date \\n(Blank if not applied)\", observation_gran=['role_title','company'], \n",
    "                     control_treatment=\"ab_split\", response_var=\"Interview Invite\", segmentation=None, \n",
    "                     verbose=True, new_col_names=['role_title','company','ab_split',\"applied_date\",'invite']):\n",
    "        \"\"\"Return a processed dataframe that shows the row wise response variable.\n",
    "            segmentation arg expects a tuple.        \n",
    "        \"\"\"\n",
    "        self.control_treatment = control_treatment\n",
    "        self.verbose=verbose\n",
    "        \n",
    "        if segmentation is None:\n",
    "            df = self.raw_data.copy()\n",
    "        else:\n",
    "            df = self.raw_data.copy()\n",
    "            df = df.loc[df.loc[:,segmentation[0]]==segmentation[1]]\n",
    "        \n",
    "        #remove rows where rm_null_rows is empty; delete anything marked with delete in that col.\n",
    "        samples = df.loc[~df.loc[:,rm_null_rows].isnull()]\n",
    "        samples = samples.loc[samples.loc[:,rm_null_rows]!=\"DELETE\"]\n",
    "        #shrink the df to only use the needed columns.\n",
    "        filter_cols = observation_gran + [control_treatment] + [rm_null_rows] + [response_var]\n",
    "        #if we have a segmentation column, then add that to our filterable cols. Otherwise don't do anything.\n",
    "        filter_cols.extend([segmentation[0]]) if segmentation else None\n",
    "        #filter\n",
    "        samples = samples[filter_cols]\n",
    "        #rename the columns\n",
    "        samples.columns = new_col_names.extend([segmentation[1]]) if segmentation else new_col_names\n",
    "        samples['response_yn'] = np.where(samples['invite'].isnull(), 0, 1)\n",
    "        \n",
    "        self.samples = samples\n",
    "        return(self)\n",
    "\n",
    "    def ctrl_treat_split(self):\n",
    "        \"\"\"Return a control and treatment df with the proper Response Rate columns.\n",
    "        this function expects the control_treament column to have 0 for control and 1 for treatment.\n",
    "        \"\"\"\n",
    "        control = self.samples.loc[self.samples.loc[:,self.control_treatment]==0]\n",
    "        treatment = self.samples.loc[self.samples.loc[:,self.control_treatment]==1]\n",
    "        ctrl_rr = calcRR(control)\n",
    "        trmt_rr = calcRR(treatment)\n",
    "        return(ctrl_rr, trmt_rr)\n",
    "    \n",
    "#Split into control and treatment.\n",
    "#control = samples.loc[samples.loc[:,'ab_split']==0]\n",
    "#treat = samples.loc[samples.loc[:,'ab_split']==1]\n",
    "\n",
    "\n",
    "\n",
    "#ctrl_rr = calcRR(control)\n",
    "#display(ctrl_rr.head(1))\n",
    "\n",
    "#trmt_rr = calcRR(treat)\n",
    "#display(trmt_rr.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f494162-d30f-48fc-8202-03e3201b2f2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>role_title</th>\n",
       "      <th>company</th>\n",
       "      <th>ab_split</th>\n",
       "      <th>applied_date</th>\n",
       "      <th>invite</th>\n",
       "      <th>response_yn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>Senior Data Scientist (Future Opportunity)</td>\n",
       "      <td>HopSkipDrive</td>\n",
       "      <td>1</td>\n",
       "      <td>12/5/2024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Tanium</td>\n",
       "      <td>1</td>\n",
       "      <td>11/20/2024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     role_title       company  ab_split  \\\n",
       "514  Senior Data Scientist (Future Opportunity)  HopSkipDrive         1   \n",
       "63                               Data Scientist        Tanium         1   \n",
       "\n",
       "    applied_date invite  response_yn  \n",
       "514    12/5/2024    NaN            0  \n",
       "63    11/20/2024    NaN            0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ab_split</th>\n",
       "      <th>sum</th>\n",
       "      <th>count</th>\n",
       "      <th>response_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>323</td>\n",
       "      <td>0.027864</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ab_split  sum  count  response_rate\n",
       "0         0    9    323       0.027864"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ab_split</th>\n",
       "      <th>sum</th>\n",
       "      <th>count</th>\n",
       "      <th>response_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>363</td>\n",
       "      <td>0.022039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ab_split  sum  count  response_rate\n",
       "0         1    8    363       0.022039"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Adding response variable\n",
    "samples = PowerAnalysis(raw_samples)\n",
    "proc_samples = samples.proc_rawdata()\n",
    "all_in_samples = proc_samples.samples\n",
    "#display(all_in_samples.sample(2))\n",
    "\n",
    "ctrl, trt = proc_samples.ctrl_treat_split()\n",
    "display(ctrl.head(1))\n",
    "display(trt.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc6a239-3e2d-490a-ad26-9f41727e9c31",
   "metadata": {},
   "source": [
    "The formula to determine the z statistic for this analysis is:\n",
    "\n",
    "$Z = \\frac{(\\hat{p_1}-\\hat{p_2}) - 0}{\\sqrt{\\hat{p}\\cdot(1-\\hat{p})\\cdot(\\frac{1}{n_1} + \\frac{1}{n_2})}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02bf9684-bd42-466f-9812-2a1a000e5d86",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z statistic: 0.4898810818653188\n"
     ]
    }
   ],
   "source": [
    "#Perform the power analysis\n",
    "def Zstat(df1, df2):\n",
    "    Z_numer = (df1['response_rate'] - df2['response_rate']) - 0\n",
    "    phat = (df1['sum'] + df2['sum'])/(df1['count'] + df2['count'])\n",
    "    Z_denom = np.sqrt(phat * (1-phat) * (1/df1['count'] + 1/df2['count']))\n",
    "    Z = Z_numer/Z_denom\n",
    "    return(Z)\n",
    "\n",
    "Z = Zstat(ctrl, trt)\n",
    "\n",
    "print(f\"Z statistic: {Z[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4a719ba2-8e5e-457a-b4a9-2272107750f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our pvalue: 0.6242180507005721\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def Pvalue(Z):\n",
    "    pvalue = 2 * (1-norm.cdf(Z))\n",
    "    return(pvalue)\n",
    "\n",
    "print(f\"Our pvalue: {Pvalue(Z)[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffffec67-908a-44dc-8593-8e57d1ca439d",
   "metadata": {},
   "source": [
    "Since our pvalue is not lower than 0.05, then we can not be confident that our results are statistically significant - ie we could have gotten this result by random chance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7a904f-d164-4833-9b39-74cb1d70fb89",
   "metadata": {},
   "source": [
    "# Where do we go from here?\n",
    "\n",
    "The best solution is to collect more samples. This, however, would take time. I've been averaging 100 applications per day if I only submit job applications.\n",
    "\n",
    "Knowing our response rate now, how long would we need to get enough sample for a stat sig result?\n",
    "\n",
    "Our formula for finding a better sample size:\n",
    "\n",
    "$n = \\frac{(Z_{\\alpha/2} + Z_{\\beta})^2 \\cdot (\\hat{p_1}(1-\\hat{p_1}) + \\hat{p_2}(1-\\hat{p_2}))}{(\\hat{p_1} - \\hat{p_2})^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0bb4d4cc-def3-4748-ab47-37ee745a1802",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our required sample size: 13491.505955749142\n",
      "We have 686 now.\n",
      "We would have to collect an additional 12805.505955749142 samples\n",
      "If 100 apps can be submitted in 1 day, it will take '128.05505955749143' days to collect the required sample\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def SampleSize(p1, p2, alpha=0.05, beta=0.2, two_tailed_alpha=True):\n",
    "    #calculate Zstatistic for alpha\n",
    "    if two_tailed_alpha:\n",
    "        alpha_zstat = norm.ppf(1-alpha/2)\n",
    "    else:\n",
    "        alpha_zstat = norm.ppf(1-alpha)\n",
    "    #calculate Zstatistic for beta.\n",
    "    beta_zstat = norm.ppf(1-beta)\n",
    "    nnumer = (alpha_zstat + beta_zstat)**2 * (p1 * (1-p1) + p2 * (1-p2))\n",
    "    ndenom = (p1 - p2)**2\n",
    "    n = nnumer/ndenom\n",
    "    return(n)\n",
    "\n",
    "n = SampleSize(p1=0.027864,p2=0.022039, alpha=0.1, beta=0.3)\n",
    "combined_sample = n*2\n",
    "print(f\"Our required sample size: {combined_sample}\")\n",
    "\n",
    "current_sample = (ctrl_rr['count'] + trmt_rr['count'])[0]\n",
    "print(f\"We have {current_sample} now.\")\n",
    "\n",
    "remaining_sample = combined_sample - current_sample\n",
    "print(f\"We would have to collect an additional {remaining_sample} samples\")\n",
    "\n",
    "print(f\"If 100 apps can be submitted in 1 day, it will take '{remaining_sample/100}' days to collect the required sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94f7d9a-f1a5-42f7-81b4-745f382ddf20",
   "metadata": {},
   "source": [
    "If I kept our alpha and beta at 10% and 30% respectively, the required sample will be too large to collect myself--an additional 12805 applications and 128 days.\n",
    "\n",
    "We can also look at the individual segments to see if there were any significant results by segment instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17985903-8401-4d31-9140-86922e8da5b4",
   "metadata": {},
   "source": [
    "# Analysis by Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed21e4b3-5848-4881-bf56-8fd07fa9d4da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
